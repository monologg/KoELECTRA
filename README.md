[í•œêµ­ì–´](./README.md) | [English](./README_EN.md)

# KoELECTRA

<p float="left" align="center">
    <img width="900" src="https://user-images.githubusercontent.com/28896432/80024445-0f444e00-851a-11ea-9137-9da2abfd553d.png" />  
</p>

[ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)ëŠ” `Replaced Token Detection`, ì¦‰ generatorì—ì„œ ë‚˜ì˜¨ tokenì„ ë³´ê³  discriminatorì—ì„œ "real" tokenì¸ì§€ "fake" tokenì¸ì§€ íŒë³„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë“  input tokenì— ëŒ€í•´ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê°€ì§€ë©°, BERT ë“±ê³¼ ë¹„êµí–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

KoELECTRAëŠ” **34GBì˜ í•œêµ­ì–´ text**ë¡œ í•™ìŠµí•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ ë‚˜ì˜¨ `KoELECTRA-Base`ì™€ `KoELECTRA-Small` ë‘ ê°€ì§€ ëª¨ë¸ì„ ë°°í¬í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë˜í•œ KoELECTRAëŠ” **Wordpiece ì‚¬ìš©**, **ëª¨ë¸ s3 ì—…ë¡œë“œ** ë“±ì„ í†µí•´ OS ìƒê´€ì—†ì´ `Transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì„¤ì¹˜í•˜ë©´ ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Updates

**April 27, 2020**

- 2ê°œì˜ Subtask (`KorSTS`, `QuestionPair`)ì— ëŒ€í•´ ì¶”ê°€ì ìœ¼ë¡œ finetuningì„ ì§„í–‰í•˜ì˜€ê³ , ê¸°ì¡´ 5ê°œì˜ Subtaskì— ëŒ€í•´ì„œë„ ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•˜ì˜€ìŠµë‹ˆë‹¤.

**June 3, 2020**

- [EnlipleAI PLM](https://github.com/enlipleai/kor_pratrain_LM)ì—ì„œ ì‚¬ìš©ëœ vocabularyë¥¼ ì´ìš©í•˜ì—¬ `KoELECTRA-v2`ë¥¼ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤. Base ëª¨ë¸ê³¼ Small ëª¨ë¸ ëª¨ë‘ `KorQuaD`ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

**October 9, 2020**

- `ëª¨ë‘ì˜ ë§ë­‰ì¹˜`ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ `KoELECTRA-v3`ë¥¼ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤. Vocabë„ `Mecab`ê³¼ `Wordpiece`ë¥¼ ì´ìš©í•˜ì—¬ ìƒˆë¡œ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤.
- `Huggingface Transformers`ì˜ `ElectraForSequenceClassification` ê³µì‹ ì§€ì› ë“±ì„ ê³ ë ¤í•˜ì—¬ ê¸°ì¡´ Subtask ê²°ê³¼ë¥¼ ìƒˆë¡œ Updateí•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ [Korean-Hate-Speech](https://github.com/kocohub/korean-hate-speech)ì˜ ê²°ê³¼ë„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

```python
from transformers import ElectraModel, ElectraTokenizer

model = ElectraModel.from_pretrained("monologg/koelectra-base-v3-discriminator")
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
```

**May 26, 2021**

- `torch<=1.4` ì—ì„œ ë¡œë”©ì´ ë˜ì§€ ì•ŠëŠ” ì´ìŠˆ í•´ê²° (ëª¨ë¸ ìˆ˜ì • í›„ ì¬ì—…ë¡œë“œ ì™„ë£Œ) ([Related Issue](https://github.com/pytorch/pytorch/issues/48915))
- huggingface hubì— `tensorflow v2` ëª¨ë¸ ì—…ë¡œë“œ (`tf_model.h5`)

**Oct 20, 2021**

- `tf_model.h5`ì—ì„œ ë°”ë¡œ ë¡œë”©í•˜ëŠ” ë¶€ë¶„ì´ ì—¬ëŸ¬ ì´ìŠˆê°€ ì¡´ì¬í•˜ì—¬ ì œê±° (`from_pt=True`ë¡œ ë¡œë”©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë˜ëŒë¦¼)

## Download Link

| Model                |                                                                     Discriminator |                                                                 Generator |                                                                                       Tensorflow-v1 |
| -------------------- | --------------------------------------------------------------------------------: | ------------------------------------------------------------------------: | --------------------------------------------------------------------------------------------------: |
| `KoELECTRA-Base-v1`  |     [Discriminator](https://huggingface.co/monologg/koelectra-base-discriminator) |     [Generator](https://huggingface.co/monologg/koelectra-base-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1AnyoxEG0nI7NZM7luy7yCL_3_8h00Oaw/view?usp=sharing) |
| `KoELECTRA-Small-v1` |    [Discriminator](https://huggingface.co/monologg/koelectra-small-discriminator) |    [Generator](https://huggingface.co/monologg/koelectra-small-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1P9ry0g9NbqUBd7X8WbHIcB627EodpDb6/view?usp=sharing) |
| `KoELECTRA-Base-v2`  |  [Discriminator](https://huggingface.co/monologg/koelectra-base-v2-discriminator) |  [Generator](https://huggingface.co/monologg/koelectra-base-v2-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1i028LR4BIa0c5z6o03gzrb67KqR_74FV/view?usp=sharing) |
| `KoELECTRA-Small-v2` | [Discriminator](https://huggingface.co/monologg/koelectra-small-v2-discriminator) | [Generator](https://huggingface.co/monologg/koelectra-small-v2-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1y_SKg9XT5dsDXXElo8DmZuUk1sRXR8p5/view?usp=sharing) |
| `KoELECTRA-Base-v3`  |  [Discriminator](https://huggingface.co/monologg/koelectra-base-v3-discriminator) |  [Generator](https://huggingface.co/monologg/koelectra-base-v3-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1L8TChlO0_bNJCHNAV3m7al-iaOt1EWkY/view?usp=sharing) |
| `KoELECTRA-Small-v3` | [Discriminator](https://huggingface.co/monologg/koelectra-small-v3-discriminator) | [Generator](https://huggingface.co/monologg/koelectra-small-v3-generator) | [Tensorflow-v1](https://drive.google.com/file/d/1qFVIaCdGXQMlS0MEQlgOWxjk-cVG75Qu/view?usp=sharing) |

## About KoELECTRA

|                   |               | Layers | Embedding Size | Hidden Size | # heads |
| ----------------- | ------------: | -----: | -------------: | ----------: | ------: |
| `KoELECTRA-Base`  | Discriminator |     12 |            768 |         768 |      12 |
|                   |     Generator |     12 |            768 |         256 |       4 |
| `KoELECTRA-Small` | Discriminator |     12 |            128 |         256 |       4 |
|                   |     Generator |     12 |            128 |         256 |       4 |

### Vocabulary

- ì´ë²ˆ í”„ë¡œì íŠ¸ì˜ ê°€ì¥ í° ëª©ì ì€ **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ìˆìœ¼ë©´ ëª¨ë¸ì„ ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ**ì´ì—ˆê³ , ì´ì— Sentencepiece, Mecabì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì› ë…¼ë¬¸ê³¼ ì½”ë“œì—ì„œ ì‚¬ìš©í•œ `Wordpiece`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
- ìì„¸í•œ ë‚´ìš©ì€ [[Wordpiece Vocabulary]](./docs/wordpiece_vocab.md) ì°¸ê³ 

|     | Vocab Len | do_lower_case |
| --- | --------: | ------------: |
| v1  |     32200 |         False |
| v2  |     32200 |         False |
| v3  |     35000 |         False |

### Data

- `v1`, `v2`ì˜ ê²½ìš° **ì•½ 14G Corpus** (2.6B tokens)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ë‰´ìŠ¤, ìœ„í‚¤, ë‚˜ë¬´ìœ„í‚¤)
- `v3`ì˜ ê²½ìš° **ì•½ 20Gì˜ ëª¨ë‘ì˜ ë§ë­‰ì¹˜**ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ì‹ ë¬¸, ë¬¸ì–´, êµ¬ì–´, ë©”ì‹ ì €, ì›¹)

### Pretraining Details

| Model        | Batch Size | Train Steps |   LR | Max Seq Len | Generator Size | Train Time |
| :----------- | ---------: | ----------: | ---: | ----------: | -------------: | ---------: |
| `Base v1,2`  |        256 |        700K | 2e-4 |         512 |           0.33 |         7d |
| `Base v3`    |        256 |        1.5M | 2e-4 |         512 |           0.33 |        14d |
| `Small v1,2` |        512 |        300K | 5e-4 |         512 |            1.0 |         3d |
| `Small v3`   |        512 |        800K | 5e-4 |         512 |            1.0 |         7d |

- `KoELECTRA-Small` ëª¨ë¸ì˜ ê²½ìš° ì› ë…¼ë¬¸ì—ì„œì˜ `ELECTRA-Small++`ì™€ **ë™ì¼í•œ ì˜µì…˜**ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

  - ì´ëŠ” ê³µì‹ ELECTRAì—ì„œ ë°°í¬í•œ Small ëª¨ë¸ê³¼ ì„¤ì •ì´ ë™ì¼í•©ë‹ˆë‹¤.
  - ë˜í•œ `KoELECTRA-Base`ì™€ëŠ” ë‹¬ë¦¬, Generatorì™€ Discriminatorì˜ ëª¨ë¸ ì‚¬ì´ì¦ˆ(=`generator_hidden_size`)ê°€ ë™ì¼í•©ë‹ˆë‹¤.

- `Batch size`ì™€ `Train steps`ì„ ì œì™¸í•˜ê³ ëŠ” **ì› ë…¼ë¬¸ì˜ Hyperparameterì™€ ë™ì¼**í•˜ê²Œ ê°€ì ¸ê°”ìŠµë‹ˆë‹¤.

  - ë‹¤ë¥¸ hyperparameterë¥¼ ë³€ê²½í•˜ì—¬ ëŒë ¤ë´¤ì§€ë§Œ ì› ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ê°€ì ¸ê°„ ê²ƒì´ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤.

- **TPU v3-8**ì„ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ê³ , GCPì—ì„œì˜ TPU ì‚¬ìš©ë²•ì€ [[Using TPU for Pretraining]](./docs/tpu_training.md)ì— ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

## KoELECTRA on ğŸ¤— Transformers ğŸ¤—

- `Transformers v2.8.0`ë¶€í„° `ElectraModel`ì„ ê³µì‹ ì§€ì›í•©ë‹ˆë‹¤.

- **Huggingface S3**ì— ëª¨ë¸ì´ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆì–´ì„œ, **ëª¨ë¸ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ì´** ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `ElectraModel`ì€ `pooled_output`ì„ ë¦¬í„´í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì œì™¸í•˜ê³  `BertModel`ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

- ELECTRAëŠ” finetuningì‹œì— `discriminator`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 1. Pytorch Model & Tokenizer

```python
from transformers import ElectraModel, ElectraTokenizer

model = ElectraModel.from_pretrained("monologg/koelectra-base-discriminator")  # KoELECTRA-Base
model = ElectraModel.from_pretrained("monologg/koelectra-small-discriminator")  # KoELECTRA-Small
model = ElectraModel.from_pretrained("monologg/koelectra-base-v2-discriminator")  # KoELECTRA-Base-v2
model = ElectraModel.from_pretrained("monologg/koelectra-small-v2-discriminator")  # KoELECTRA-Small-v2
model = ElectraModel.from_pretrained("monologg/koelectra-base-v3-discriminator")  # KoELECTRA-Base-v3
model = ElectraModel.from_pretrained("monologg/koelectra-small-v3-discriminator")  # KoELECTRA-Small-v3
```

### 2. Tensorflow v2 Model

```python
from transformers import TFElectraModel

model = TFElectraModel.from_pretrained("monologg/koelectra-base-v3-discriminator", from_pt=True)
```

### 3. Tokenizer Example

```python
>>> from transformers import ElectraTokenizer
>>> tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
>>> tokenizer.tokenize("[CLS] í•œêµ­ì–´ ELECTRAë¥¼ ê³µìœ í•©ë‹ˆë‹¤. [SEP]")
['[CLS]', 'í•œêµ­ì–´', 'EL', '##EC', '##TRA', '##ë¥¼', 'ê³µìœ ', '##í•©ë‹ˆë‹¤', '.', '[SEP]']
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'í•œêµ­ì–´', 'EL', '##EC', '##TRA', '##ë¥¼', 'ê³µìœ ', '##í•©ë‹ˆë‹¤', '.', '[SEP]'])
[2, 11229, 29173, 13352, 25541, 4110, 7824, 17788, 18, 3]
```

## Result on Subtask

**configì˜ ì„¸íŒ…ì„ ê·¸ëŒ€ë¡œ í•˜ì—¬ ëŒë¦° ê²°ê³¼ì´ë©°, hyperparameter tuningì„ ì¶”ê°€ì ìœ¼ë¡œ í•  ì‹œ ë” ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

ì½”ë“œ ë° ìì„¸í•œ ë‚´ìš©ì€ [[Finetuning]](./finetune/README.md) ì°¸ê³ 

### Base Model

|                       | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) | **Korean-Hate-Speech (Dev)**<br/>(F1) |
| :-------------------- | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: | :-----------------------------------: |
| KoBERT                |       89.59        |         87.92          |       81.25        |        79.62         |           81.59           |            94.85            |         51.75 / 79.15         |                 66.21                 |
| XLM-Roberta-Base      |       89.03        |         86.65          |       82.80        |        80.23         |           78.45           |            93.80            |         64.70 / 88.94         |                 64.06                 |
| HanBERT               |       90.06        |         87.70          |       82.95        |        80.32         |           82.73           |            94.72            |         78.74 / 92.02         |               **68.32**               |
| KoELECTRA-Base        |       90.33        |         87.18          |       81.70        |        80.64         |           82.00           |            93.54            |         60.86 / 89.28         |                 66.09                 |
| KoELECTRA-Base-v2     |       89.56        |         87.16          |       80.70        |        80.72         |           82.30           |            94.85            |         84.01 / 92.40         |                 67.45                 |
| **KoELECTRA-Base-v3** |     **90.63**      |       **88.11**        |     **84.45**      |      **82.24**       |         **85.53**         |          **95.25**          |       **84.83 / 93.45**       |                 67.61                 |

### Small Model

|                        | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) | **Korean-Hate-Speech (Dev)**<br/>(F1) |
| :--------------------- | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: | :-----------------------------------: |
| DistilKoBERT           |       88.60        |         84.65          |       60.50        |        72.00         |           72.59           |            92.48            |         54.40 / 77.97         |                 60.72                 |
| KoELECTRA-Small        |       88.83        |         84.38          |       73.10        |        76.45         |           76.56           |            93.01            |         58.04 / 86.76         |                 63.03                 |
| KoELECTRA-Small-v2     |       88.83        |         85.00          |       72.35        |        78.14         |           77.84           |            93.27            |         81.43 / 90.46         |                 60.14                 |
| **KoELECTRA-Small-v3** |     **89.36**      |       **85.40**        |     **77.45**      |      **78.60**       |         **80.79**         |          **94.85**          |       **82.11 / 91.13**       |               **63.07**               |

## Acknowledgement

KoELECTRAì€ **Tensorflow Research Cloud (TFRC)** í”„ë¡œê·¸ë¨ì˜ Cloud TPU ì§€ì›ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ `KoELECTRA-v3`ëŠ” **ëª¨ë‘ì˜ ë§ë­‰ì¹˜**ì˜ ë„ì›€ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## Citation

ì´ ì½”ë“œë¥¼ ì—°êµ¬ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ì¸ìš©í•´ì£¼ì„¸ìš”.

```
@misc{park2020koelectra,
  author = {Park, Jangwon},
  title = {KoELECTRA: Pretrained ELECTRA Model for Korean},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/monologg/KoELECTRA}}
}
```

## Reference

- [ELECTRA](https://github.com/google-research/electra)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [Tensorflow Research Cloud](https://www.tensorflow.org/tfrc?hl=ko)
- [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA/blob/master/README_EN.md)
- [Enliple AI Korean PLM](https://github.com/enlipleai/kor_pratrain_LM)
- [ëª¨ë‘ì˜ ë§ë­‰ì¹˜](https://corpus.korean.go.kr/)
