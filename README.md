[í•œêµ­ì–´](./README.md) | [English](./README_EN.md)

# KoELECTRA

<p float="left" align="center">
    <img width="900" src="https://user-images.githubusercontent.com/28896432/80024445-0f444e00-851a-11ea-9137-9da2abfd553d.png" />  
</p>

[ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)ëŠ” `Replaced Token Detection`, ì¦‰ generatorì—ì„œ ë‚˜ì˜¨ tokenì„ ë³´ê³  discriminatorì—ì„œ "real" tokenì¸ì§€ "fake" tokenì¸ì§€ íŒë³„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë“  input tokenì— ëŒ€í•´ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê°€ì§€ë©°, BERT ë“±ê³¼ ë¹„êµí–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

KoELECTRAëŠ” **14GBì˜ í•œêµ­ì–´ text** (96M sentences, 2.6B tokens)ë¡œ í•™ìŠµí•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ ë‚˜ì˜¨ `KoELECTRA-Base`ì™€ `KoELECTRA-Small` ë‘ ê°€ì§€ ëª¨ë¸ì„ ë°°í¬í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë˜í•œ KoELECTRAëŠ” **Wordpiece ì‚¬ìš©**, **ëª¨ë¸ s3 ì—…ë¡œë“œ** ë“±ì„ í†µí•´ OS ìƒê´€ì—†ì´ `Transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì„¤ì¹˜í•˜ë©´ ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Updates

**April 27, 2020** - 2ê°œì˜ Subtask (`KorSTS`, `QuestionPair`)ì— ëŒ€í•´ ì¶”ê°€ì ìœ¼ë¡œ finetuningì„ ì§„í–‰í•˜ì˜€ê³ , ê¸°ì¡´ 5ê°œì˜ Subtaskì— ëŒ€í•´ì„œë„ ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•˜ì˜€ìŠµë‹ˆë‹¤.

## About KoELECTRA

|                   |               | Layers | Embedding Size | Hidden Size | # heads | Size |
| ----------------- | ------------: | -----: | -------------: | ----------: | ------: | ---: |
| `KoELECTRA-Base`  | Discriminator |     12 |            768 |         768 |      12 | 423M |
|                   |     Generator |     12 |            768 |         256 |       4 | 134M |
| `KoELECTRA-Small` | Discriminator |     12 |            128 |         256 |       4 |  53M |
|                   |     Generator |     12 |            128 |         256 |       4 |  53M |

### Vocabulary

ì´ë²ˆ í”„ë¡œì íŠ¸ì˜ ê°€ì¥ í° ëª©ì ì€ **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ìˆìœ¼ë©´ ëª¨ë¸ì„ ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ**ì´ì—ˆê³ , ì´ì— Sentencepiece, Mecabì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì› ë…¼ë¬¸ê³¼ ì½”ë“œì—ì„œ ì‚¬ìš©í•œ `Wordpiece`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- Vocabì˜ ì‚¬ì´ì¦ˆëŠ” `32200`ê°œë¡œ `[unused]` í† í° 200ê°œë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.
- Cased (`do_lower_case=False`)ë¡œ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ [[Wordpiece Vocabulary]](./docs/wordpiece_vocab.md) ì°¸ê³ 

### Pretraining Details

- Dataì˜ ê²½ìš° ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ **14Gì˜ Corpus**(2.6B tokens)ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. (ì „ì²˜ë¦¬ ê´€ë ¨ ë‚´ìš©ì€ [[Preprocessing]](./docs/preprocessing.md) ì°¸ê³ )

  |       Model       | Batch Size | Train Steps | Learning Rate | Max Seq Len | Generator Size |
  | :---------------: | ---------: | ----------: | ------------: | ----------: | -------------: |
  | `KoELECTRA-Base`  |        256 |        700K |          2e-4 |         512 |           0.33 |
  | `KoELECTRA-Small` |        512 |        300K |          5e-4 |         512 |            1.0 |

- `KoELECTRA-Small` ëª¨ë¸ì˜ ê²½ìš° ì› ë…¼ë¬¸ì—ì„œì˜ `ELECTRA-Small++`ì™€ **ë™ì¼í•œ ì˜µì…˜**ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

  - ì´ëŠ” ê³µì‹ ELECTRAì—ì„œ ë°°í¬í•œ Small ëª¨ë¸ê³¼ ì„¤ì •ì´ ë™ì¼í•©ë‹ˆë‹¤.
  - ë˜í•œ `KoELECTRA-Base`ì™€ëŠ” ë‹¬ë¦¬, Generatorì™€ Discriminatorì˜ ëª¨ë¸ ì‚¬ì´ì¦ˆ(=`generator_hidden_size`)ê°€ ë™ì¼í•©ë‹ˆë‹¤.

- `Batch size`ì™€ `Train steps`ì„ ì œì™¸í•˜ê³ ëŠ” **ì› ë…¼ë¬¸ì˜ Hyperparameterì™€ ë™ì¼**í•˜ê²Œ ê°€ì ¸ê°”ìŠµë‹ˆë‹¤.

  - ë‹¤ë¥¸ hyperparameterë¥¼ ë³€ê²½í•˜ì—¬ ëŒë ¤ë´¤ì§€ë§Œ ì› ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ê°€ì ¸ê°„ ê²ƒì´ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤.

- **TPU v3-8**ì„ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ê³ , Base ëª¨ë¸ì€ **ì•½ 7ì¼**, Small ëª¨ë¸ì€ **ì•½ 3ì¼**ì´ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤.

  - GCPì—ì„œì˜ TPU ì‚¬ìš©ë²•ì€ [[Using TPU for Pretraining]](./docs/tpu_training.md)ì— ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

## KoELECTRA on ğŸ¤— Transformers ğŸ¤—

- `Transformers v2.8.0`ë¶€í„° `ElectraModel`ì„ ê³µì‹ ì§€ì›í•©ë‹ˆë‹¤.

- **Huggingface S3**ì— ëª¨ë¸ì´ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆì–´ì„œ, **ëª¨ë¸ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ì´** ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `ElectraModel`ì€ `pooled_output`ì„ ë¦¬í„´í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì œì™¸í•˜ê³  `BertModel`ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

- ELECTRAëŠ” finetuningì‹œì— `discriminator`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from transformers import ElectraModel, ElectraTokenizer

# KoELECTRA-Base
model = ElectraModel.from_pretrained("monologg/koelectra-base-discriminator")
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")

# KoELECTRA-Small
model = ElectraModel.from_pretrained("monologg/koelectra-small-discriminator")
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-discriminator")
```

```python
>>> from transformers import ElectraTokenizer
>>> tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")
>>> tokenizer.tokenize("[CLS] í•œêµ­ì–´ ELECTRAë¥¼ ê³µìœ í•©ë‹ˆë‹¤. [SEP]")
['[CLS]', 'í•œêµ­ì–´', 'E', '##L', '##EC', '##T', '##RA', '##ë¥¼', 'ê³µìœ ', '##í•©ë‹ˆë‹¤', '.', '[SEP]']
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'í•œêµ­ì–´', 'E', '##L', '##EC', '##T', '##RA', '##ë¥¼', 'ê³µìœ ', '##í•©ë‹ˆë‹¤', '.', '[SEP]'])
[2, 18429, 41, 6240, 15229, 6204, 20894, 5689, 12622, 10690, 18, 3]
```

## Result on Subtask

**configì˜ ì„¸íŒ…ì„ ê·¸ëŒ€ë¡œ í•˜ì—¬ ëŒë¦° ê²°ê³¼ì´ë©°, hyperparameter tuningì„ ì¶”ê°€ì ìœ¼ë¡œ í•  ì‹œ ë” ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

ì½”ë“œ ë° ìì„¸í•œ ë‚´ìš©ì€ [[Finetuning]](./finetune/README.md) ì°¸ê³ 

### Base Model

|                    | Size  | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |
| :----------------- | :---: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |
| KoBERT             | 351M  |       89.63        |         86.11          |       80.65        |        79.00         |           79.64           |            93.93            |         52.81 / 80.27         |
| XLM-Roberta-Base        | 1.03G |       89.49        |         86.26          |     **82.95**      |        79.92         |           79.09           |            93.53            |         64.70 / 88.94         |
| HanBERT            | 614M  |       90.16        |       **87.31**        |       82.40        |      **80.89**       |         **83.33**         |            94.19            |       **78.74 / 92.02**       |
| **KoELECTRA-Base** | 423M  |     **90.21**      |         86.87          |       81.90        |        80.85         |           83.21           |          **94.20**          |         61.10 / 89.59         |

`KoELECTRA-Base`ì˜ ê²½ìš° `KoBERT`ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, `HanBERT`ì™€ ì¼ë¶€ Taskì—ì„œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

### Small Model

|                     | Size | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |
| :------------------ | :--: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |
| DistilKoBERT        | 108M |       88.41        |       **84.13**        |       62.55        |        70.55         |           73.21           |            92.48            |         54.12 / 77.80         |
| **KoELECTRA-Small** | 53M  |     **88.76**      |         84.11          |     **74.15**      |      **76.27**       |         **77.00**         |          **93.01**          |       **58.13 / 86.82**       |

`KoELECTRA-Small`ì˜ ê²½ìš° ì „ë°˜ì ìœ¼ë¡œ `DistilKoBERT`ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## Acknowledgement

KoELECTRAì€ Tensorflow Research Cloud (TFRC) í”„ë¡œê·¸ë¨ì˜ Cloud TPU ì§€ì›ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## Reference

- [ELECTRA](https://github.com/google-research/electra)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [Tensorflow Research Cloud](https://www.tensorflow.org/tfrc?hl=ko)
- [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA/blob/master/README_EN.md)
