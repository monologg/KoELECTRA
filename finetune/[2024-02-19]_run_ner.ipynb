{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# import numpy as np\n",
    "import torch\n",
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "# from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from attrdict import AttrDict\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from huggingface_hub import notebook_login, create_repo, delete_repo, Repository, upload_file, delete_file\n",
    "\n",
    "# from transformers import (\n",
    "#     AdamW,\n",
    "#     get_linear_schedule_with_warmup\n",
    "# )\n",
    "\n",
    "from src import (\n",
    "    CONFIG_CLASSES,\n",
    "    TOKENIZER_CLASSES,\n",
    "    MODEL_FOR_TOKEN_CLASSIFICATION,\n",
    "    init_logger,\n",
    "    set_seed,\n",
    "    # compute_metrics,\n",
    "    # show_ner_report\n",
    ")\n",
    "\n",
    "from processor import ner_load_and_cache_examples as load_and_cache_examples\n",
    "from processor import ner_tasks_num_labels as tasks_num_labels\n",
    "from processor import ner_processors as processors\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from run_ner import train, train_v2, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 로그인\n",
    "# 토큰링크: https://huggingface.co/settings/tokens\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 레포지토리 로드\n",
    "repo = Repository(path=\"path_to_your_repo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../../../../data/KoELECTRA_config/naver-ner/koelectra-small-v3.json\"\n",
    "# Read from config file and make args\n",
    "with open(config_path) as f:\n",
    "    args = AttrDict(json.load(f))\n",
    "logger.info(\"Training/evaluation parameters {}\".format(args))\n",
    "\n",
    "# args.output_dir = os.path.join(args.ckpt_dir, args.output_dir)\n",
    "\n",
    "init_logger()\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenizer 학습\n",
    "\n",
    "vocab_path = \"../../../../models/KoELECTRA/repo/vocab.txt\"\n",
    "train_file_path = \"../../../../data/KoELECTRA/naver-ner/train.tsv\"\n",
    "vocab_size = 80000\n",
    "limit_alphabet = 6000\n",
    "model_path = \"../../../../models/KoELECTRA/koelectra-small-v3-naver-ner-ckpt/checkpoint-best/vocab.txt\"\n",
    "move_vocab_path = \"../../../../models/KoELECTRA/vocab/vocab_%s_%s_20240219.txt\"%(vocab_size, limit_alphabet)\n",
    "un_used_num = 1000\n",
    "\n",
    "# `lowercase=False`로 할 시 `strip_accent=False`로 해야함\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab=vocab_path,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True, # Must be False if cased model\n",
    "    lowercase=True,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[train_file_path],\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# 원본 vocab.txt 파일 옮기기\n",
    "shutil.move(vocab_path, move_vocab_path)\n",
    "\n",
    "vocab_list = [i[0] for i in sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])]\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for vocab in vocab_list:\n",
    "        f.write(vocab+\"\\n\")\n",
    "    \n",
    "    for i in un_used_num:\n",
    "        f.write(\"[unused%s]\\n\"%i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'delete_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 파일 삭제\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdelete_file\u001b[49m(\n\u001b[1;32m      3\u001b[0m     REPO_FILE_PATH,\n\u001b[1;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mNAMESPACE\u001b[38;5;241m/\u001b[39mREPO_NAME,\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'delete_file' is not defined"
     ]
    }
   ],
   "source": [
    "### vocab 파일 삭제 및 업데이트\n",
    "\n",
    "# 파일을 직접 업로드\n",
    "upload_file(\n",
    "    LOCAL_FILE_PATH,\n",
    "    path_in_repo=REPO_FILE_PATH,\n",
    "    repo_id=NAMESPACE/REPO_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"../../../../models/huggingface\"\n",
    "processor = processors[args.task](args)\n",
    "labels = processor.get_labels()\n",
    "config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=tasks_num_labels[args.task],\n",
    "    id2label={str(i): label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model = MODEL_FOR_TOKEN_CLASSIFICATION[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU or CPU\n",
    "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "model.to(args.device)\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/14/2024 04:55:38 - INFO - processor.ner -   Loading features from cached file ../../../../data/KoELECTRA/cached_naver-ner_koelectra-small-v3-discriminator_128_train\n",
      "02/14/2024 04:55:42 - INFO - processor.ner -   Loading features from cached file ../../../../data/KoELECTRA/cached_naver-ner_koelectra-small-v3-discriminator_128_test\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\") if args.train_file else None\n",
    "dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\") if args.dev_file else None\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\") if args.test_file else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dev_dataset == None:\n",
    "    args.evaluate_test_during_training = True  # If there is no dev dataset, only use testset\n",
    "\n",
    "if args.do_train:\n",
    "    global_step, tr_loss = train_v2(args, model, train_dataset, dev_dataset, test_dataset)\n",
    "    logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/14/2024 04:55:44 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../models/KoELECTRA/koelectra-small-v3-naver-ner-ckpt/checkpoint-best']\n",
      "02/14/2024 04:55:44 - INFO - run_ner -   ***** Running evaluation on test dataset (best step) *****\n",
      "02/14/2024 04:55:44 - INFO - run_ner -     Num examples = 9000\n",
      "02/14/2024 04:55:44 - INFO - run_ner -     Eval Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../../models/KoELECTRA/koelectra-small-v3-naver-ner-ckpt/checkpoint-best']\n",
      " |████████████████████████████████████████| 100.00% [71/71 00:02<00:00]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/14/2024 04:55:47 - INFO - run_ner -   ***** Eval results on test dataset *****\n",
      "02/14/2024 04:55:47 - INFO - run_ner -     f1 = 0.8594204130194611\n",
      "02/14/2024 04:55:47 - INFO - run_ner -     loss = 0.2844202115502156\n",
      "02/14/2024 04:55:47 - INFO - run_ner -     precision = 0.8544969074255585\n",
      "02/14/2024 04:55:47 - INFO - run_ner -     recall = 0.8644009846827133\n",
      "02/14/2024 04:55:48 - INFO - run_ner -   \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.58      0.61      0.60       394\n",
      "         ANM       0.77      0.78      0.78       701\n",
      "         CVL       0.84      0.84      0.84      5758\n",
      "         DAT       0.92      0.93      0.92      2521\n",
      "         EVT       0.77      0.78      0.78      1094\n",
      "         FLD       0.60      0.68      0.63       228\n",
      "         LOC       0.85      0.86      0.85      2126\n",
      "         MAT       0.17      0.17      0.17        12\n",
      "         NUM       0.92      0.93      0.92      5590\n",
      "         ORG       0.88      0.87      0.87      4086\n",
      "         PER       0.88      0.89      0.88      4426\n",
      "         PLT       0.27      0.18      0.21        34\n",
      "         TIM       0.86      0.93      0.90       314\n",
      "         TRM       0.73      0.74      0.74      1964\n",
      "\n",
      "   micro avg       0.85      0.86      0.86     29248\n",
      "   macro avg       0.72      0.73      0.72     29248\n",
      "weighted avg       0.85      0.86      0.86     29248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "if args.do_eval:\n",
    "    checkpoints = list(os.path.dirname(c) for c in\n",
    "                        # sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True), key=lambda path_with_step: list(map(int, re.findall(r\"\\d+\", path_with_step)))[-1]))\n",
    "                        sorted(glob.glob(args.output_dir + \"/**/\" + \"training_args.bin\", recursive=True), key=lambda path_with_step: list(map(int, re.findall(r\"\\d+\", path_with_step)))[-1]))\n",
    "\n",
    "    print(checkpoints)\n",
    "    if not args.eval_all_checkpoints:\n",
    "        checkpoints = checkpoints[-1:]\n",
    "    else:\n",
    "        logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split(\"-\")[-1]\n",
    "        model = MODEL_FOR_TOKEN_CLASSIFICATION[args.model_type].from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        result = evaluate(args, model, test_dataset, mode=\"test\", global_step=global_step)\n",
    "        result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as f_w:\n",
    "        if len(checkpoints) > 1:\n",
    "            for key in sorted(results.keys(), key=lambda key_with_step: (\n",
    "                    \"\".join(re.findall(r'[^_]+_', key_with_step)),\n",
    "                    int(re.findall(r\"_\\d+\", key_with_step)[-1][1:])\n",
    "            )):\n",
    "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "        else:\n",
    "            for key in sorted(results.keys()):\n",
    "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델파일들을 repo 경로에 복사\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_path = \"../../../../models/KoELECTRA/koelectra-small-v3-naver-ner-ckpt/checkpoint-best/*\"\n",
    "# 대상 파일 경로 (복사될 위치와 파일 이름)\n",
    "target_path = \"../../../../models/KoELECTRA/repo/\"\n",
    "\n",
    "# print(glob(source_path))\n",
    "for source_file in glob(source_path):\n",
    "    filename = os.path.basename(source_file)\n",
    "    target_file = os.path.join(target_path, filename)\n",
    "    print(source_file, target_file)\n",
    "    # 파일 복사\n",
    "    shutil.copyfile(source_file, target_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원격 저장소에 업로드\n",
    "commit_message = 'Initial commit'\n",
    "repo.git_add()\n",
    "repo.git_commit(commit_message)\n",
    "repo.git_push()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
